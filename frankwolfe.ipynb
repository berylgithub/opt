{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Analysis of Zeroth-Order Frank-Wolfe Variants [Sahu et al] on Black Box settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert a mathematical formula we use the dollar symbol, as follows: <br>\n",
    "Euler's identity: $ e^{i \\pi} + 1 = 0 $\n",
    "To isolate and center the formulas and enter in math display mode, we use 2 dollars symbol:\n",
    "$$\n",
    "...\n",
    "$$\n",
    "Euler's identity: $$ e^{i \\pi} + 1 = 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pylab as plt\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t615.3251790206386\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of solution: 0.11%\n"
     ]
    }
   ],
   "source": [
    "'''from Fabian Pedregosa'''\n",
    "def FW(alpha, max_iter=200, tol=1e-8):\n",
    "    # .. initial estimate, could be any feasible point ..\n",
    "    x_t = sparse.dok_matrix((n_features, 1))\n",
    "    trace = []  # to keep track of the gap\n",
    "\n",
    "    # .. some quantities can be precomputed ..\n",
    "    Atb = A.T.dot(b)\n",
    "    for it in range(max_iter):\n",
    "        # .. compute gradient. Slightly more involved than usual because ..\n",
    "        # .. of the use of sparse matrices ..\n",
    "        Ax = x_t.T.dot(A.T).ravel()\n",
    "        grad = (A.T.dot(Ax) - Atb)\n",
    "        # .. the LMO results in a vector that is zero everywhere except for ..\n",
    "        # .. a single index. Of this vector we only store its index and magnitude ..\n",
    "        idx_oracle = np.argmax(np.abs(grad))\n",
    "        mag_oracle = alpha * np.sign(-grad[idx_oracle])\n",
    "        g_t = x_t.T.dot(grad).ravel() - grad[idx_oracle] * mag_oracle\n",
    "        trace.append(g_t)\n",
    "        if g_t <= tol:\n",
    "            break\n",
    "        q_t = A[:, idx_oracle] * mag_oracle - Ax\n",
    "        step_size = min(q_t.dot(b - Ax) / q_t.dot(q_t), 1.)\n",
    "        x_t = (1. - step_size) * x_t\n",
    "        x_t[idx_oracle] = x_t[idx_oracle] + step_size * mag_oracle\n",
    "    return x_t, np.array(trace)\n",
    "\n",
    "n_samples, n_features = 1000, 10000\n",
    "#n_samples, n_features = 100, 1000\n",
    "A, b = datasets.make_regression(n_samples, n_features)\n",
    "# .. plot evolution of FW gap ..\n",
    "sol, trace = FW(.5 * n_features)\n",
    "plt.plot(trace)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('FW gap')\n",
    "plt.title('FW on a Lasso problem')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "sparsity = np.mean(sol.toarray().ravel() != 0)\n",
    "print('Sparsity of solution: %s%%' % (sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([], dtype=float64)\n",
      "     fun: -11.428571428000076\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 4\n",
      "   slack: array([1.9874058e-10, 2.5518343e-10])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-1.14285714,  2.57142857])\n",
      "11.428571428000076\n"
     ]
    }
   ],
   "source": [
    "'''Linear Programming'''\n",
    "# f(x0,x1) = -x0 + 4x1\n",
    "f = lambda x: -x[0] + 4*x[1]\n",
    "c = np.array([-1, 4])\n",
    "A = [[-3, 1], [1, 2]]\n",
    "b = [6, 4]\n",
    "\n",
    "#A = np.array([[-3, 1]])\n",
    "#b = 6\n",
    "\n",
    "x0_bounds = (None, None)\n",
    "x1_bounds = (-3, None)\n",
    "res = linprog(-c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]) #there is also A_eq and b_eq which is Ax = b constraints\n",
    "print(res)\n",
    "print(f(res[\"x\"]))\n",
    "#print(linear_program_oracle(c, A, b, [(None, None), (-3, None)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWSA [-32.  -8.]\n",
      "RDSA [ 0.00561439 -0.27489941]\n",
      "IRDSA [-23.33818602   3.44828786]\n",
      "KWSA [0. 0.]\n",
      "RDSA [-0. -0.]\n",
      "IRDSA [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "'''Helper functions, for gradients, oracles, etc'''\n",
    "def grad_KWSA(F, x, c, d, *f_args):\n",
    "    '''\n",
    "    f_args is the arguments of F(.), used if there are complicated functions which reqiures more parameter than just x \n",
    "    '''\n",
    "    grad = 0\n",
    "    e = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        e[i] = 1 #cb-vector\n",
    "        grad += ((F(x + c*e, *f_args) - F(x, *f_args))/c)*e #calculate the gradient\n",
    "        e[i] = 0 #reset cb-vector\n",
    "    return grad\n",
    "\n",
    "def grad_RDSA(F, x, c, d, *f_args):\n",
    "    z = np.random.normal(0, 1, size=(d)) #mean = 0, std = 1\n",
    "    grad = ((F(x + c*z, *f_args) - F(x, *f_args))/c)*z\n",
    "    return grad\n",
    "\n",
    "def grad_IRDSA(F, x, c, d, m=5, *f_args):\n",
    "    grad = 0\n",
    "    for i in range(m):\n",
    "        z = np.random.normal(0, 1, size=(d)) #mean = 0, std = 1\n",
    "        grad += ((F(x + c*z, *f_args) - F(x, *f_args))/c)*z\n",
    "    return grad/m\n",
    "\n",
    "def linear_program_oracle(coef, A_ub, b_ub, bounds):\n",
    "    '''\n",
    "    f(x) = c^Tx\n",
    "    Ax <= b\n",
    "    x within bounds \n",
    "    coef = [c_x0, c_x1, ....] coefficient of the equation\n",
    "    A = [[a0_x0, a0_x1, ...],[a1_x0, a1_x1, ...]] ineq constraint matrix\n",
    "    b = [b0, b1, ....] ineq constraint vector\n",
    "    bounds = [(lb_x0, ub_x0), (lb_x1, ub_x1),....]\n",
    "    '''\n",
    "    res = linprog(coef, A_ub=A_ub, b_ub=b_ub, bounds=bounds)\n",
    "    return res[\"x\"] \n",
    "\n",
    "# gradient test:\n",
    "'''\n",
    "f = lambda x: x[0]**2 + x[1]**2\n",
    "c = 1e-10\n",
    "x = np.array([1,2])\n",
    "'''\n",
    "f = lambda x: x[0]**4 - 32*x[0] + x[1]**2 - 8*x[1]\n",
    "c = 1e-10\n",
    "x = np.array([0,0])\n",
    "print(\"KWSA\", grad_KWSA(f, x, c, len(x)))\n",
    "print(\"RDSA\", grad_RDSA(f, x, c, len(x)))\n",
    "print(\"IRDSA\", grad_IRDSA(f, x, c, len(x)))\n",
    "x = np.array([2,4]) #x = (2,4) is the global minimum, hence gradient = 0\n",
    "print(\"KWSA\", grad_KWSA(f, x, c, len(x)))\n",
    "print(\"RDSA\", grad_RDSA(f, x, c, len(x)))\n",
    "print(\"IRDSA\", grad_IRDSA(f, x, c, len(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Franke Wolfe Algorithm and its variants from [Sahu et al] paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Standard frank wolfe algorfithm'''\n",
    "def frank_wolfe(F, x, LPO_var, alpha=0.1, max_iter=200, *f_args):\n",
    "    '''\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    f_args is the arguments of F(.), used if there are complicated functions which reqiures more parameter than just x \n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    x_out = np.copy(x) #so that the global input x wont be changed\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_KWSA(F, x_out, 1e-10, dim, *f_args)\n",
    "        v = linear_program_oracle(grad, A, b, bounds)\n",
    "        x_out += alpha*(v - x_out)\n",
    "    return x_out\n",
    "\n",
    "def frank_wolfe_away(F, x, LPO_var, alpha=0.1, max_iter=200, *f_args):\n",
    "    '''\n",
    "    away version of Frank Wolfe Algorithm\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    alpha_bar = 0 #placeholder\n",
    "    x_out = np.copy(x)\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_KWSA(F, x_out, 1e-10, dim, *f_args)\n",
    "        v_FW = linear_program_oracle(grad, A, b, bounds)\n",
    "        v_AS = linear_program_oracle(-grad, A, b, bounds) #away step\n",
    "        d_FW = v_FW - x_out; d_AS = x_out - v_AS\n",
    "        grad_FW = np.dot(grad, d_FW); grad_AS = np.dot(grad, d_AS)\n",
    "        if grad_FW <= grad_AS:\n",
    "            d = d_FW\n",
    "            alpha_bar = 1\n",
    "        else:\n",
    "            d = d_AS\n",
    "            alpha_bar = 1 #temporary\n",
    "        x_out += alpha*d\n",
    "    return x_out\n",
    "    \n",
    "    \n",
    "'''Zeroth Order Frank Wolfe Algorithms [Sahu et al]'''\n",
    "'''Stochastic Gradient Free Frank Wolfe - Convex'''\n",
    "def SGF_frank_wolfe_cvx(F, x, LPO_var, max_iter=200, m=10, mode=\"KWSA\", *f_args):\n",
    "    '''\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    x_out = np.copy(x)\n",
    "    t = np.arange(max_iter) # sequence of t = 0,1,....max_iter\n",
    "    gamma = 2/(t + 8) # initialize gamma sequences\n",
    "    # rho and c sequences:\n",
    "    rho = c = None\n",
    "    if mode == \"KWSA\":\n",
    "        rho = 4/((t+8)**(2/3))\n",
    "        c = 2/(dim**(1/2) * ((t + 8)**(1/3)))\n",
    "    elif mode == \"IRDSA\":\n",
    "        rho = 4/(((1 + (dim/m))**(1/3)) * ((t+8)**(2/3)))\n",
    "        c = 2*np.sqrt(m)/(dim**(3/2) * (t+8)**(1/3))\n",
    "    elif mode == \"RDSA\":\n",
    "        rho = 4/(dim**(1/3) * (t+8)**(2/3))\n",
    "        c = 2/(dim**(3/2) * (t+8)**(1/3))\n",
    "    d = np.zeros(dim) #initial value d_0 = 0\n",
    "    x_mean = 0 #for xT\n",
    "    for i in t: #loop\n",
    "        grad = 0 #placeholder\n",
    "        if mode == \"KWSA\":\n",
    "            grad = grad_KWSA(F, x_out, c[i], dim, *f_args)\n",
    "        elif mode == \"IRDSA\":\n",
    "            grad = grad_IRDSA(F, x_out, c[i], dim, m, *f_args)\n",
    "        elif mode == \"RDSA\":\n",
    "            grad = grad_RDSA(F, x_out, c[i], dim, *f_args)\n",
    "        d = (1-rho[i])*d + rho[i]*grad #used for linear programming oracle (LPO)\n",
    "        v = linear_program_oracle(d, A, b, bounds) ### LPO\n",
    "        x_out = (1-gamma[i])*x_out + gamma[i]*v\n",
    "        x_mean += x_out\n",
    "    x_mean /= max_iter #(sum of x)/T\n",
    "    return x_mean\n",
    "\n",
    "'''Stochastic Gradient Free Frank Wolfe - NonConvex'''\n",
    "def SGF_frank_wolfe_noncvx(F, x, LPO_var, max_iter=200, m=10, *f_args):\n",
    "    '''\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    x_out = np.copy(x)\n",
    "    t = np.arange(max_iter) # sequence of t = 0,1,....max_iter\n",
    "    gamma = 1/(max_iter**(3/4)) # initialize gamma sequences\n",
    "    # rho and c sequences:\n",
    "    rho = 4/((1+dim/m)**(1/3) * (t+8)**(2/3))\n",
    "    c = 2*np.sqrt(m)/(dim**(3/2) * (t+8)**(1/3))\n",
    "    d = np.zeros(dim) #initial value d_0 = 0\n",
    "    for i in t: #loop\n",
    "        grad = grad_IRDSA(F, x_out, c[i], dim, m, *f_args)\n",
    "        d = (1-rho[i])*d + rho[i]*grad #used for linear programming oracle (LPO)\n",
    "        v = linear_program_oracle(d, A, b, bounds) ### LPO\n",
    "        x_out = (1-gamma)*x_out + gamma*v\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<li>Simple function for testing <br>\n",
    "$x^2 + y^2 + z^2$ <br>\n",
    "$-1 \\leq (x,y,z) \\leq 1$ <br>\n",
    "minimizer value:  $F(0,0,0) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{min x, F(x)} of Standard Frank Wolfe = [ 0.05263158 -0.05263158  0.05263158] 0.008310249311776426\n",
      "{min x, F(x)} of Frank Wolfe Away = [ 0.05263158 -0.05263158  0.05263158] 0.008310249311776426\n",
      "{min x, F(x)} of SGF Cvx Frank Wolfe = [ 0.01864734 -0.05487923 -0.00125604] 0.0033610305958132088\n",
      "{min x, F(x)} of SGF Non-Cvx Frank Wolfe = [-0.13396247  0.00948164 -0.0383157 ] 0.019503938360854525\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.sum(x**2)\n",
    "x = np.array([1., -1., 1.]) #random x0 within feasible reigon\n",
    "A = None\n",
    "b = None\n",
    "bounds = [(-1, 1)]*len(x)\n",
    "LPO_var = {\"A\":A, \"b\":b, \"bounds\":bounds}\n",
    "m=3 #for IRDSA gradient\n",
    "\n",
    "x_out = frank_wolfe(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Standard Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = frank_wolfe_away(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Frank Wolfe Away =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_cvx(f, x, LPO_var, max_iter=200, m=m, mode=\"IRDSA\") #looks like m corresponds to the dimension size of the problem to be more accurate\n",
    "print(\"{min x, F(x)} of SGF Cvx Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, max_iter=200, m=m)\n",
    "print(\"{min x, F(x)} of SGF Non-Cvx Frank Wolfe =\", x_out, f(x_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Maximization test<br>\n",
    "$F(x,y,z) = x^2 + y^2 + z^2$ <br>\n",
    "$-1 \\leq (x,y,z) \\leq 1$ <br>\n",
    "the combinations of $\\{-1,1\\} \\in \\mathbb{Z}^3$ are the max values, e.g: $F(-1,1,1) = -3,\\quad f = -F = 3$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{min x, F(x)} of Standard Frank Wolfe = [1. 1. 1.] 2.9999999957669528\n",
      "{min x, F(x)} of Frank Wolfe Away = [842333.73448923 842333.73448923 842335.23367636] 2128580886409.752\n",
      "{min x, F(x)} of SGF Cvx Frank Wolfe = [ 0.97101449  0.82173913 -0.7857971 ] 2.2356014282713703\n",
      "{min x, F(x)} of SGF Non-Cvx Frank Wolfe = [0.97650838 0.97563416 0.97183358] 2.8498911184981144\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "f = lambda x: -np.sum(x**2)\n",
    "x = np.array([0., 0., 0.]) #random x0 within feasible reigon\n",
    "A = None\n",
    "b = None\n",
    "bounds = [(-1, 1)]*len(x)\n",
    "LPO_var = {\"A\":A, \"b\":b, \"bounds\":bounds}\n",
    "m=3 #for IRDSA gradient\n",
    "\n",
    "x_out = frank_wolfe(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Standard Frank Wolfe =\", x_out, -f(x_out))\n",
    "x_out = frank_wolfe_away(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Frank Wolfe Away =\", x_out, -f(x_out))\n",
    "x_out = SGF_frank_wolfe_cvx(f, x, LPO_var, max_iter=200, m=m, mode=\"IRDSA\") #looks like m corresponds to the dimension size of the problem to be more accurate\n",
    "print(\"{min x, F(x)} of SGF Cvx Frank Wolfe =\", x_out, -f(x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, max_iter=200, m=m)\n",
    "print(\"{min x, F(x)} of SGF Non-Cvx Frank Wolfe =\", x_out, -f(x_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Function from: http://www.math.udel.edu/~angell/Opt/FW.pdf <br>\n",
    "minimzer value: $ F(1,695, 1,914) = -57.63$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{min x, F(x)} of Standard Frank Wolfe = [1.582 2.253] -57.31468003018529\n",
      "{min x, F(x)} of Frank Wolfe Away = [2.178 3.739] -63.12670634345123\n",
      "{min x, F(x)} of SGF Cvx Frank Wolfe = [1.449 2.352] -55.23591041598563\n",
      "{min x, F(x)} of SGF Non-Cvx Frank Wolfe = [1.549 2.195] -56.55772389787758\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x[0]**4 - 32*x[0] + x[1]**2 - 8*x[1]\n",
    "x = np.array([0., 0.])\n",
    "A = [[1, -1], [3, 1]]\n",
    "b = [1, 7]\n",
    "bounds = [(0, None), (0, None)]\n",
    "LPO_var = {\"A\":A, \"b\":b, \"bounds\":bounds}\n",
    "m=5 #for IRDSA gradient\n",
    "x_out = frank_wolfe(f, x, LPO_var)\n",
    "print(\"{min x, F(x)} of Standard Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = frank_wolfe_away(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Frank Wolfe Away =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_cvx(f, x, LPO_var, max_iter=200, m=m, mode=\"IRDSA\") #looks like m corresponds to the dimension size of the problem to be more accurate\n",
    "print(\"{min x, F(x)} of SGF Cvx Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, max_iter=200, m=m)\n",
    "print(\"{min x, F(x)} of SGF Non-Cvx Frank Wolfe =\", x_out, f(x_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li> Regression test, treated as blackbox function.<br>\n",
    "$\\frac{1}{2}||Ax - b||^2_2$ <br>\n",
    "$||x||_1 \\leq \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal function value (residual) =  1.5450629689658737e-26\n",
      "f(x) of Standard Frank Wolfe = 605575.647055823\n",
      "||coef - x_out|| = 126.42898810755354\n",
      "f(x) of SGF Non-Cvx FW = 633214.47815463\n",
      "||coef - x_out|| = 128.89743769100548\n",
      "f(lasso_coef) = 612.388767301812\n",
      "||coef-lasso_coef|| = 4.1026964337542315\n"
     ]
    }
   ],
   "source": [
    "#Regression test, treated as black box\n",
    "float_formatter = \"{:.3f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "n_samples = 100; n_features = 100 #A is n_samples x n_features matrix, x is the variable to be optimized, b is the target\n",
    "A, b, coef = datasets.make_regression(n_samples, n_features, coef=True, random_state=0) #coef is the optimizer value\n",
    "f = lambda x,A,b: 0.5*np.square(np.linalg.norm(np.dot(A,x) - b))\n",
    "#f = lambda x,A,b: (0.5/b.shape[0])*np.square(np.linalg.norm(np.dot(A,x) - b)) + np.linalg.norm(x,1)\n",
    "#print(\"optimal coefficients = \",coef)\n",
    "print(\"optimal function value (residual) = \", f(coef, A, b)) #the optimal value should be near 0\n",
    "A_ub = np.ones((1,n_features)) #L1 norm of x is <= 0.5*n_features\n",
    "b_ub = 0.5*n_features\n",
    "#A_ub = None\n",
    "#b_ub = None\n",
    "\n",
    "'''\n",
    "#gradient test:\n",
    "#print(\"KWSA\",grad_KWSA(f, np.zeros(n_features), 1e-10, n_features, A,b))\n",
    "#print(\"RDSA\",grad_KWSA(f, np.zeros(n_features), 1e-10, n_features, A,b))\n",
    "#print(\"IRDSA\",grad_IRDSA(f, np.zeros(n_features), 1e-10, n_features, 5, A,b))\n",
    "'''\n",
    "\n",
    "\n",
    "LPO_var = {\"A\":A_ub, \"b\":b_ub, \"bounds\":np.array([[None, None]]*n_features)}\n",
    "m=6\n",
    "x = np.random.normal(0, 1, size=(n_features))\n",
    "x_out = frank_wolfe(f, x, LPO_var, 0.1, 200, A, b)\n",
    "#print(\"x of Standard Frank Wolfe =\", x_out)\n",
    "print(\"f(x) of Standard Frank Wolfe =\", f(x_out, A, b))\n",
    "print(\"||coef - x_out|| =\", np.linalg.norm(coef-x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, 200, m, A, b)\n",
    "#print(\"x of SGF Non-Cvx FW =\", x_out)\n",
    "print(\"f(x) of SGF Non-Cvx FW =\", f(x_out, A, b))\n",
    "print(\"||coef - x_out|| =\", np.linalg.norm(coef-x_out))\n",
    "\n",
    "# actual lasso fit:\n",
    "from sklearn.linear_model import Lasso\n",
    "alpha=1\n",
    "dense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\n",
    "dense_lasso.fit(A, b)\n",
    "print(\"f(lasso_coef) =\", f(dense_lasso.coef_,A,b))\n",
    "print(\"||coef-lasso_coef|| =\", np.linalg.norm(coef-dense_lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal value is -15.220912605552863\n",
      "A solution x is\n",
      "[-1.10133381 -0.16360111 -0.89734939  0.03216603  0.6069123  -1.12687348\n",
      "  1.12967856  0.88176638  0.49075229  0.8984822 ]\n",
      "A dual solution is\n",
      "[6.98805172e-10 6.11756416e-01 5.28171747e-01 1.07296862e+00\n",
      " 3.93759300e-09 2.30153870e+00 4.25704434e-10 7.61206896e-01\n",
      " 8.36906030e-09 2.49370377e-01 1.30187120e-09 2.06014070e+00\n",
      " 3.22417207e-01 3.84054343e-01 1.59493839e-09]\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "# Generate a random non-trivial linear program.\n",
    "m = 15\n",
    "n = 10\n",
    "np.random.seed(1)\n",
    "s0 = np.random.randn(m)\n",
    "lamb0 = np.maximum(-s0, 0)\n",
    "s0 = np.maximum(s0, 0)\n",
    "x0 = np.random.randn(n)\n",
    "A = np.random.randn(m, n)\n",
    "b = A @ x0 + s0\n",
    "c = -A.T @ lamb0\n",
    "\n",
    "# Define and solve the CVXPY problem.\n",
    "x = cp.Variable(n)\n",
    "prob = cp.Problem(cp.Minimize(c.T@x),\n",
    "                 [A @ x <= b])\n",
    "prob.solve()\n",
    "\n",
    "# Print result.\n",
    "print(\"\\nThe optimal value is\", prob.value)\n",
    "print(\"A solution x is\")\n",
    "print(x.value)\n",
    "print(\"A dual solution is\")\n",
    "print(prob.constraints[0].dual_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
