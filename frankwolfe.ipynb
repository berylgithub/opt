{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Analysis of Zeroth-Order Frank-Wolfe Variants [Sahu et al] on Black Box settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert a mathematical formula we use the dollar symbol, as follows: <br>\n",
    "Euler's identity: $ e^{i \\pi} + 1 = 0 $\n",
    "To isolate and center the formulas and enter in math display mode, we use 2 dollars symbol:\n",
    "$$\n",
    "...\n",
    "$$\n",
    "Euler's identity: $$ e^{i \\pi} + 1 = 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pylab as plt\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xl4VPXZ//H3nZAQIEBAJCBEwi6LgoTFpSKxLrhUrHUBrbvFpbh0Fau1tv7s+rTPUxVrrSLaKilaV4pSi1HEjcUFooAgggQQRBAIe+D+/TEn7RizTcjMmUw+r+s6F3POnOWTk+HcOct8v+buiIiI1FVa2AFERKRxUeEQEZGYqHCIiEhMVDhERCQmKhwiIhITFQ4REYmJCoeIfImZXWpmc2p4/2UzuzKRmSS5qHBI3JjZSjPbaWZlUcMhZjbTzH4cNV8XM/NqpnUKJ/1XmdkoMysNO4dI2FQ4JN6+4e7ZUcNaYDZwfNQ8I4ElVUxb5u6fJjBrk2Bm6WFnkMZNhUPCMBs41swqPn/HAf8HDK00bXZVC5tZmpndamarzGyDmT1iZm2D9/KDM5VLzOwTM9toZrdUF8TMTjezd8xsq5mtNrPb6/MD1bQeM8sys7+Z2edm9oWZzTOz3OC9S81shZltM7OPzezC2n7GKrY9ysxKzewnwc+7smI9wftTzOxPZjbDzLYDhWbWNljnZ8E2bo3a98FidreZbTGzJWb29Rp+9svNbLGZbQ7OJrtFvedmdq2ZLQt+xjvMrKeZvRHsq2lmllmffS4hcncNGuIyACuBE6uY3hzYCRwZjJcAPYDXKk27uJr1Xg4sD5bJBp4E/hq8lw848BegBTAI2A30q2Zdo4DDifwRdQSwHjirhnlLY10PcBXwHNASSAcKgDZAK2Ar0DeYrzMwoLafsZptlwN/CPbt8cD2qPVOAbYAxwb5soBHgGeA1sE++xC4Ipj/0mB93wMygPOD5dsH778MXBm8PivI2Q9oBtwKvB6VzYFng593QPC7mBX8XG2BD4BLwv6saohtCD2AhtQdgsJRBnwRDE9HvfcycAPQvuJgDPw6atp+oFs1650FXBs13hfYGxy4KgpH16j35wJj65j5/4D/rea9agtHTesJisDrwBGV5mkV7JdvAS3q+jNWk6scaBU1bRrw0+D1FOCRqPfSgwN4/6hpVwEvB68vBdYCVmkfXhT1u6soHM9XFJxgPA3YUfG7C34Xx0a9vwC4KWr898D/hf1Z1RDboEtVEm9nuXtOMJwVNX02kfsYxwEVT/DMiZq22t1XVbPOQ4Do91YRKRq5UdOi743sIPJX+1eY2QgzKw4u2WwBrgY61O1Hq/N6/grMBIrMbK2Z/dbMMtx9O5G/5q8G1pnZP83ssBh+xmibg/VFz39I1PjqqNcdgMwq1t8lanyNB0f2atZXoRvwx+AS3BfAJsAqrWt91OudVYxX+buR5KXCIWGZTaRAjAReDaa9RuRyykiqub8RWEvkgFXhUCJ/ca+vevYaPUbkUkqeu7cF7iNy4Guw9bj7Xnf/ubv3B44BzgAuDt6b6e4nEblMtYTIJTaI/WdsZ2atKs2/Nmo8ughsJHL2Unn9a6LGu5iZVXo/en0VVgNXRf1xkOPuLdz99WpySgpQ4ZCwvA7kAN8mKBzuvhn4LJhWU+GYCnzPzLqbWTbwS+Dv7l5ejxytgU3uvsvMhgMX1LZAcLM7erCa1mNmhWZ2ePA001YiB+19ZpZrZmcGB/zdRC7r7TuAn/HnZpZpZscRKU6PVzWTu+8jcinrTjNrHdzM/j7wt6jZOgLXm1mGmZ1L5B7GjCpWdx9ws5kNCH7WtsH8ksJUOCQU7r6DyPXu5kRuhFd4lchBq6bCMZnI5Z/ZwMfALuC6eka5FviFmW0DbiNyQK1JFyKXV6KHnrWspxPwBJGisRh4hchBOg34AZG/5DcRual9bT1/xk+BzcG6HgWudvclNcx/HZEb6CuIXCJ8LNhmhbeA3kTOTu4EznH3zyuvxN2fAn5D5DLcViK/y1Nr2K6kAPvyZUwRaWzMbBTwN3fvGnYWaRp0xiEiIjFR4RARkZjoUpWIiMREZxwiIhKTZmEHiIcOHTp4fn5+vZbdvn07rVq1qn3GBEvWXJC82ZQrNsmaC5I3W6rlWrBgwUZ3P7jWGcP+6no8hoKCAq+v4uLiei8bT8mayz15sylXbJI1l3vyZku1XMB8V5MjIiLS0FQ4REQkJiocIiISExUOERGJiQqHiIjERIVDRERiosIhIiIxUeGI8kLJOp79aA9Fcz9hy469YccREUlKKhxRZi3ewJPL9jLxyUWc+sfZFC/dwI499ekbSEQkdaVkkyP19btzB3HyQZto12MQP3j8PS57aB5pBheO6MYtp/cjKyM97IgiIqFT4agkI80Ymt+e5284jjnLNvLKh5/x1zdXMW/lJu65YAi9OmaHHVFEJFS6VFWNlpnNOHlAJ+785uE8dOkwNmzbzTfunsPj81fjaopeRJowFY46KDysI8/fcByD8tryoycW8oPH32PffhUPEWmakr5wmNmhZvasmU02s4lh5chtk8WjVx7F9Sf04sm31/CHF5eGFUVEJFShFI6gCGwws5JK00eb2VIzWx5VJPoA/3T3y4H+CQ8bJT3N+P7JfRk7LI9JxR9RvHRDmHFEREIR1hnHFGB09AQzSwcmAacSKRDjzKw/8A4w1sxeAooTnLNKt585gG4HteS+lz8KO4qISMKFUjjcfTawqdLk4cByd1/h7nuAImAMcBnwM3c/ATg9sUmrlpWRztlHdmXuyk18umVX2HFERBLKwnpCyMzygenuPjAYPwcY7e5XBuMXASOA+4DbgY1Ambv/sJr1jQfGA+Tm5hYUFRXVK1dZWRnZ2bU/cruubD83z9nJuMMyOSU/o17bikeuMCRrNuWKTbLmguTNlmq5CgsLF7j70FpnrEs3gfEYgHygJGr8XOCBqPGLgLvrs+5EdR172h9n+5n3zKn3tmKRrF1UuidvNuWKTbLmck/ebKmWi0bYdWwpkBc13hVYG1KWOjlz0CG8t/oL5izbGHYUEZGESabCMQ/obWbdzSwTGAs8G3KmGn37qG707pjN9UXvsG7LzrDjiIgkRFiP404F3gD6mlmpmV3h7uXABGAmsBiY5u7vh5Gvrlo1b8afvl3A7r37OPve13nlw8/CjiQiEndhPVU1zt07u3uGu3d19weD6TPcvY+793T3O8PIFqteHbOZOv4oWjVvxqUPzWXBqs1hRxIRiatkulTVaB3RNYdnvnss7VpmctesZWHHERGJKxWOBtKqeTOuPK47r3z4Ge+u/iLsOCIicaPC0YAuPjqfnJYZ3D9b3ygXkdSlwtGAsps34/TDO/Py0s/YXb4v7DgiInGhwtHAvt6vIzv27OOtFZVbVBERSQ0qHA3smJ4dyMpI46UlajlXRFKTCkcDy8pI55ieHZi1ZL16ChSRlKTCEQcnHNaR1Zt28uhbn6h4iEjKUeGIgzGDD2FE9/bc+nQJp901h9++sIStu/aGHUtEpEGocMRB66wMpn7nKO785kBaN2/Gn2ev4KpHFuhJKxFJCSoccZKWZlw4ohvTrj6a/zn3CN5Y8Tkn/WE2p9/1KovXbQ07nohIvTULO0BT8M0ju7J3n/NCyae8uuwznnpnDf06twk7lohIveiMI0HOG5rH5EuHMSy/Pa8sVSu6ItJ4qXAk2Mg+B7N0/Tb1VS4ijZYKR4Id3+dgAGYv01mHiDROSV84zCzNzO40s7vN7JKw8xyowzq1pmPr5ur0SUQarbB6AJxsZhvMrKTS9NFmttTMlpvZxGDyGKALsJdIv+SNmplxYv9cZpZ8yktL1ocdR0QkZmGdcUwBRkdPMLN0YBJwKtAfGGdm/YG+wBvu/n3gmgTnjIuJpx5Gv85tuPpvb/PjJ97j5aVq10pEGo+wuo6dDVRuPnY4sNzdV7j7HqCIyNlGKVDRH2tKfIOuTVYGj1w+nFF9DuZfH6zniofns7Fsd9ixRETqxMJqS8nM8oHp7j4wGD8HGO3uVwbjFwEjgB8DdwM7gCXuPqma9Y0HxgPk5uYWFBUV1StXWVkZ2dnZ9Vq2Pkq37efW13by7X6ZnNgtI2lyxSJZsylXbJI1FyRvtlTLVVhYuMDdh9Y6o7uHMgD5QEnU+LnAA1HjFwF312fdBQUFXl/FxcX1Xra+TvnfV3zMPXNqnCeMXHWVrNmUKzbJmss9ebOlWi5gvtfhGJtMT1WVAnlR412BtSFlSahvHtmFd1d/wcqN28OOIiJSq2QqHPOA3mbW3cwygbHAsyFnSogzBx+CGUyd90nYUUREahXW47hTgTeAvmZWamZXuHs5MAGYCSwGprn7+2HkS7TObVtw+uGd+dsbq9i8fU/YcUREahTWU1Xj3L2zu2e4e1d3fzCYPsPd+7h7T3e/M4xsYbnuhN5s37OPh177OOwoIiI1SqZLVU1a306tGT2gE3959WOmzlXPgSKSvFQ4ksjPxwxgcF4ONz+5iCF3vMg1f1vA6k07wo4lIvIl6o8jieS2yeLRK0fw3MK1vLZ8IzMWfcqpf3yVscPyyNlVzqiwA4qIoMKRdNLSjDGDuzBmcBeuO6E3tz1TwiNvrmJP+X4Kjvyco3seFHZEEWnidKkqieW1b8lDlw3n3dtOIqe58cdZH4YdSUREhaMxaJnZjNO6Z/Dmik1qjl1EQqfC0UiMymtGpzZZXDJ5Lt+4ew5rvtgZdiQRaaJUOBqJzHTj6e8eyy2n9WPlxu1c/dcF7NqbEo0Fi0gjo8LRiHRqm8V3RvbgD+cPZtGaLfz2haVhRxKRJkiFoxE6qX8uZxzRmWfeXcO+/fqioIgklgpHI3XygE58vn0P75V+EXYUEWliVDgaqeN7H0x6mjFrsfotF5HEUuFopNq2zGBot3bMWqz+ykUksVQ4GrET++Wy5NNtLNTlKhFJIBWORuzMwYeQ26Y5Y+9/k8fe+oSy3eVhRxKRJkCFoxHLbZPFsxO+Rt9OrfnJU4sYfue/eXPF52HHEpEU1ygKh5m1MrMFZnZG2FmSTW6bLP5x9TE8cfXR5LbJ4gfT3mPrrr1hxxKRFBZW17GTzWyDmZVUmj7azJaa2XIzmxj11k3AtMSmbDzS0oyh+e35/XmDWLdlJ+f/+U0mPPY2Nz2xkCcWlIYdT0RSTFhnHFOA0dETzCwdmAScCvQHxplZfzM7EfgA0HOntRhyaDvuOGsg6WnwwdqtvPD+p0z8x0I2bN0VdjQRSSEWVhelZpYPTHf3gcH40cDt7n5KMH5zMGs20IpIMdkJfNPd91exvvHAeIDc3NyCoqKieuUqKysjOzu7XsvGU31yrd++n5te3clZvTI4q1dmnJKl1j5LBOWKXbJmS7VchYWFC9x9aK0zunsoA5APlESNnwM8EDV+EXBP1PilwBl1WXdBQYHXV3Fxcb2Xjaf65rpk8ls+7P+96HvK9zVsoCipts/iTblil6zZUi0XMN/rcIxNppvjVsW0/5wOufsUd5+ewDwp4eKju7Fh226u+usCZqsvDxFpAMlUOEqBvKjxrsDakLKkjFF9OjKhsBcLS7/g4slzufnJhezco+bYRaT+kqlwzAN6m1l3M8sExgLPhpyp0UtLM354Sl9en/h1rhnVk6J5q7n5yYVhxxKRRiysx3GnAm8Afc2s1MyucPdyYAIwE1gMTHP398PIl4oym6Vx0+jDuP6E3jz97lpeKPk07Egi0kg1C2Oj7j6umukzgBkJjtOkTDihF/9evJ6b/rGQrIw0RvXtGHYkEWlkkulSlSRARnoa9144hE5tsrj0oXlMm7867Egi0siocDRB3Q5qxTMTjqWgWzv+Z+ZS9V0uIjFR4WiisjLS+f5JfdiwbTePq1kSEYmBCkcTdkzPgxhyaA73Fi9n9aYdYccRkUZChaMJMzNuOb0/ZbvKOe2uV3l9+cawI4lII6DC0cQVdGvHjBuOo32rTO6csbiieRcRkWqpcAh57Vty5XE9eH/tVhaWbgk7jogkORUOAeCswYfQMjOdx976JOwoIpLkVDgEgNZZGYwZfAhPvbOG8/78Bs8vWhd2JBFJUioc8h/XHN+L0QM7serz7fzmhSW63yEiVVLhkP849KCW3DXuSH5wUl9Wfr6D93S/Q0SqoMIhXzH68E5kNkvj6XfWhB1FRJKQCod8RZusDE7ql8tz763VFwNF5CtUOKRKF444lM079nDcb4v51p9e55l317BjT3nYsUQkCYTSrLokv2N6deCVHxUyY9E6ps79hBuK3iUrI40T++Vy4YhuHNWjPWZV9fYrIqku6QuHmZ0FnA50BCa5+79CjtRk5LVvyVXH9+Q7x/XgzY8/5/lFn/Lse2uZvnAdJ/XP5f+dNZDcNllhxxSRBAurB8DJZrbBzEoqTR9tZkvNbLmZTQRw96fd/TvApcD5IcRt8tLSjGN6duCOswby1k++zk9OO4zZH37G0b+axXn3vcHmXfvDjigiCVRr4TCzHmb2nJltDA72z5hZjwPc7hRgdKXtpAOTgFOB/sA4M+sfNcutwfsSoqyMdMaP7MnMG0cy4YTezF25iVfX6N6HSFNitX3Jy8zeJHLAnhpMGgtc5+4jDmjDZvnAdHcfGIwfDdzu7qcE4zcHs/46GF5093/XsL7xwHiA3NzcgqKionrlKisrIzs7u17LxlOy5rp1zg6ym+1n4lHJly1Z95lyxS5Zs6VarsLCwgXuPrTWGd29xgF4q4ppb9a2XB3Wmw+URI2fAzwQNX4RcA9wPbAAuA+4ui7rLigo8PoqLi6u97LxlKy5bn1qkff9yXTfW74v7Chfkaz7TLlil6zZUi0XMN/rcIytyz2OYjObaGb5ZtbNzH4M/NPM2ptZ+9jqWY2qekTH3f0udy9w96vd/b4G3J40gGHd27NrH3ywbmvYUUQkQeryVFXFDemrKk2/HHDgQO93VCgF8qLGuwJrG2jdEifD8yN/O8z9eBNHdM0JOY2IJEKthcPduyciCDAP6G1m3YE1RO6lXJCgbUs9dWqbRceWxisffsaFI7rRIjM97EgiEmd1+h6HmQ0k8qTTfx7ad/dH6rtRM5sKjAI6mFkp8DN3f9DMJgAzgXRgsru/X99tSOIceXA6M5dtZNDP/0VBt3Z8c0gXzhuaV/uCItIo1Vo4zOxnRA7y/YEZRB6XnQPUu3C4+7hqps8ItiGNyLl9M7ng60N4bflGipds4MdPLOSQti34Wu8OYUcTkTioy83xc4CvA5+6+2XAIKB5XFNJo9IszTi+z8H85LR+PHfd1+jeoRU3P7WQnXv2hR1NROKgLoVjp7vvB8rNrA2wgYa7IS4pJisjnV+dfTirN+1kyusrw44jInFQl8Ix38xygL8Q+T7F28DcuKaSRu2oHgdR0K0dz7yr/jxEUlGthcPdr3X3L4LvUJwEXBJcshKp1hlHdGbJp9tYvmFb2FFEpIHVpa2qIRUD0B5oZmY9zSzpW9aV8Jx+eGfM4Ln31oUdRUQaWF0O/vcCQ4CFRL7dPTB4fZCZXe1q5lyq0LFNFiO6t+exuZ+wfusuzhuWx5BD24UdS0QaQF3ucawEjnT3oe5eABwJlAAnAr+NYzZp5L5zXA/aZDXjn4vWccFf3mTOso1hRxKRBlCXwnFY9Bfx3P0DIoVkRfxiSSr4er9cZv1gFMU/HEX+Qa24fMo8ps79JOxYInKA6lI4lprZn8zs+GC4F/jQzJoDe+OcT1JAh+zmFI0/ihE92nPzk4uY+I+F7Nqr73iINFZ1ucdxKXAtcCORexxzgB8SKRqFcUsmKSWnZSZTLhvOH15cyqTij1iwajMF3dpxeNe2nDKgEx2y9Z1SkcaiLo0c7gR+HwyVlTV4IklZ6WnGj045jCO65nBv8XJe/GA9RfNW8+sZS5j5vZEcktMi7IgiUgd6pFYS7pQBnThlQCfcnVeXbeTiyXN546PP+VZB17CjiUgd1OUeh0hcmBnH9upAdvNmvLv6i7DjiEgdVVs4zGywmVXVK59Ig0lPM47o2laFQ6QRqemM4wFgo5m9aGa3m9nJQSOHIg1qcF4Oi9dt1ZNWIo1EtYXD3YcS6cr1TmAPcD2wzMzeCx7JTQgza2VmD5vZX8zswkRtVxJncF4O5fudkjVbwo4iInVQ4z0Od9/h7i8DfwT+F5gEtAJGH8hGzWyymW0ws5JK00eb2VIzW25mE4PJZwNPuPt3gDMPZLuSnAYfGumrXJerRBqHap+qMrMLgGOAwcBuIn2CvwV8zd0/PcDtTgHuIaoXQTNLJ1KYTgJKgXlm9izQFVgUzKZrGSmoY+ssuuS04KHXVgLQtV0LBuXl0LmtHs8VSUbm7lW/YVYGLAHuA2a7+4cNumGzfGC6uw8Mxo8Gbnf3U4Lxm4NZS4HN7j7dzIrcfWw16xsPjAfIzc0tKCoqqleusrIysrOz67VsPCVrLmiYbIs+K+eJZXtZtXU/AK0y4MfDsujWJj3UXPGgXLFL1myplquwsHBBcJuiZu5e5QCkE2kVdwLwGJFOnKYDtwAnVLdcXQcgHyiJGj8HeCBq/CIiZyWtgIeAPwEX1mXdBQUFXl/FxcX1XjaekjWXe8NmK928w+d+/Lkf86tZfsTtM33GwrW+f//+0HM1JOWKXbJmS7VcwHyvwzG2ppvj+9z9bXe/x90vAE4DngcuA16MuZTVrqpHf93dt7v7Ze5+jbs/GoftShLpktOCYfntKRp/FJ3bZnHNo29z7aNvU75vf9jRRCRQ0/c4jjCzq83sETNbTuQex0jgbmBEHLKUEnmKq0JXYG0ctiONQF77lky/7mv86JS+PF/yKT9/7oOKM1ERCVlNTY5MAV4jcpbxU3dfFecs84DeZtYdWAOMBS6I8zYliTVLT+O7hb3YunMvf569gj6dWnPRUd3CjiXS5NVUOL4Zr2JhZlOBUUAHMysFfubuD5rZBGAmkfsrkz2qHxBpum4afRgfrt/GL557n4GHtOFI9SQoEqqavsfxVMULM/tHQ27U3ce5e2d3z3D3ru7+YDB9hrv3cfee7n5nQ25TGq+0NON/zx9Mbpssrn30bT4v2x12JJEmrabCEX2zuke8g4jUJKdlJvd9u4DPt+/hxr+/y779ut8hEpaaCodX81okFAO7tOWOMQN4ddlGzr3vdRas2hx2JJEmqabCMcjMtprZNuCI4PVWM9tmZlsTFVAk2vnDDuW35xzBmi92Mvb+N1iwalPYkUSanJq+x5Hu7m3cvbW7NwteV4yrlVwJzXlD85h5Y6THwKv++jbrtuwMO5JIk6KOnKRRymmZyQMXD2XnnnJ+9PhCfcdDJIFUOKTR6p3bmomn9WPO8o1Mm7867DgiTYYKhzRqFw4/lOHd23Pzk4s4a9JrvLrss7AjiaQ8FQ5p1NLSjD9dOIQJhb3YsnMvVzw8n9c/2hh2LJGUpsIhjd5B2c35/sl9efKaY8g/qCVXTJnPtPmrdd9DJE5UOCRltGuVyaNXHsXgvBx+/MRCXlhZHnYkkZSkwiEp5eDWzfnblSMo6NaO19eqcIjEgwqHpJz0NOPEfrms3raf9Vt3hR1HJOWocEhKGtmnAwCzP9RTViINTYVDUlK/Tm1ok2nMXqYnrEQaWk39cYg0WmlpxuEd0nl12WfMWbaRg7IzyW7ejLz2LcOOJtLoNYrCYWZnAacDHYFJ7v6vkCNJIzCsUzqvr9vNtx986z/T7jhroHoRFDlAcS8cZjYZOAPY4O4Do6aPBv5IpLe/B9z919Wtw92fBp42s3bA/wAqHFKrwR2b8c5PR7JozRbKdpXz+IJSbnumhNLNOzimZweO73Nw2BFFGqVE3OOYAoyOnmBm6cAk4FSgPzDOzPqb2eFmNr3S0DFq0VuD5UTqJKdlJsf1PphTD+/MvRcOobBvR/78ygoumTyXF0rWhR1PpFGyRHy71szygekVZxxmdjRwu7ufEozfDODuv6pmeQN+Dbzo7v+uZp7xwHiA3NzcgqKionplLSsrIzs7u17LxlOy5oLkzVZdrl3lzi1zdnJwS2Pi8BZJkytsyZoLkjdbquUqLCxc4O5Da53R3eM+APlASdT4OUQuT1WMXwTcU8Py1wMLgPuAq2vbXkFBgddXcXFxvZeNp2TN5Z682WrKNal4mXe7abp/+OnWxAUKNMb9FbZkzZZquYD5XodjeliP41oV06o99XH3u9y9wN2vdvf74phLmojzh+aRmZ7G5Nc+DjuKSKMTVuEoBfKixrsCa0PKIk3QQdnNGTc8j6lzV6svD5EYhVU45gG9zay7mWUCY4FnQ8oiTdQtp/fnuN4duPnJRVz58HxeW64vC4rURdwLh5lNBd4A+ppZqZld4e7lwARgJrAYmObu78c7i0i0zGZp/OnbBVx0VDcWrfmCq/66gG279oYdSyTpxb1wuPs4d+/s7hnu3tXdHwymz3D3Pu7e093vjHcOkapkN2/G7WcO4P6LhlK2u5wnFpSGHUkk6amtKhFgUF4ORx6aw8Ovr2T/fnUAJVITFQ6RwGXHdmfl5zt4bO4nYUcRSWqNoq0qkUQ4bWAnHu/dgdueKWH15h24wzkFXemT2zrsaCJJRWccIoFm6Wncf9FQhuW358+vrOCBV1fwrXtf5/WP9LSVSDQVDpEoLTLTKRp/FO/edhJzbjqBTm2zuOqRBezauy/saCJJQ4VDpBIzI6dlJofktOC2b/Rn2+5y9SQoEkWFQ6QGR/U4iLYtMni+5NOwo4gkDRUOkRpkpKdxcv9c/v3BenaX63KVCKhwiNTqtMM7s213Ob97YSkvfrCe4iUbWFj6hb5lLk2WHscVqcWxvTow5NAcHpjzMQ/M+W9ruq0y05ly+XCG5bcPMZ1I4qlwiNQis1kaT157LJu272H1ph3sc+ezbbv5zfNLuHzKPKZddTT9OrcJO6ZIwuhSlUgdtW+VyaC8HIYc2o5TBnTir1eOoHmzdO785+Kwo4kklAqHSD11yWnBJUd3Y87yjXy8cXvYcUQSRoVD5ACcPyyP9DRjqtq3kiZE9zhEDkDHNlmc3D+XormfkGbG2UO6qG0rSXmN4ozDzFqZ2QIzOyPsLCKVXf/13hyS04IH56xg3P1vsvaLnWFHEomruBYOM5vzi4ubAAAQVUlEQVRsZhvMrKTS9NFmttTMlpvZxDqs6iZgWnxSihyYfp3b8MKNI3n+hpHsLt/PNX9bwJ7y/WHHEombeJ9xTAFGR08ws3RgEnAq0B8YZ2b9zexwM5teaehoZicCHwDr45xV5ID06pjN7845gvdKt/DoW6vCjiMSN3G9x+Hus80sv9Lk4cByd18BYGZFwBh3/xXwlUtRZlYItCJSZHaa2Qx3159zkpRGD+zE0T0O4p6XlnPu0Dyym+s2oqQec49vN5lB4Zju7gOD8XOA0e5+ZTB+ETDC3SfUsp5LgY3uPr2a98cD4wFyc3MLioqK6pW3rKyM7Ozsei0bT8maC5I3W1i5Vnyxj1+8uYuRXZtxwWGZZDWzpMhVm2TNBcmbLdVyFRYWLnD3obXO6O5xHYB8oCRq/Fzggajxi4C7G3KbBQUFXl/FxcX1XjaekjWXe/JmCzPXz54p8W43TfeCO170Tz7f/qX3tL9il6zZUi0XMN/rcIwN46mqUiAvarwrsDaEHCJxc/uZA/jHNUezddde7nvlo7DjiDSoMArHPKC3mXU3s0xgLPBsCDlE4qqgW3vOPrILTywoZWPZ7rDjiDSYeD+OOxV4A+hrZqVmdoW7lwMTgJnAYmCau78fzxwiYbnyuB7sLt/PL/+5mLc/2czqTTvYuz++9xVF4i3eT1WNq2b6DGBGPLctkgx6dczmvKFdmTa/lCffWQNAm0y4tc1qzhnSlbQ0q2UNIslHzwqKxNlvzxnE907qw/trtrJp+x7un1XCj59YyGNvfcIvxgzgiK45YUcUiYkKh0gCdG7bgs5tWwDQoWw5m9v05lfPL2HMpNc4f2get32jPy0z9d9RGodG0VaVSCpJM+NbBV156YfHc/mx3Smat5opr68MO5ZInalwiISkTVYGPz2jP/06t2HOso1hxxGpMxUOkZB9rddBzF+5mZ179oUdRaROVDhEQnZsrw7s2befeSs3hR1FpE5UOERCNrx7ezLT03htuS5XSeOgwiESspaZzRjSLYeXlmxQPx7SKKhwiCSBcwvyWLahjEsmz2XLjr1hxxGpkQqHSBL4VkFX/nDeIBas2syFD77JFzv2hB1JpFr6xpFIkjh7SFfatcrkqkcWMOp/XmbffueyY7vz/ZP6hB1N5Et0xiGSRAr7duShy4YxsvfBDM7L4a5Zy/j7vE/CjiXyJTrjEEkyx/bqwLG9OlC+bz+XPzyfW54qoW2LTEYP7BR2NBFAZxwiSatZehqTLjiSI7q2ZcJjb/PMu2vCjiQCqHCIJLXWWRk8fPlwhhzajhuK3uUnTy1i1959uDubtusGuoQj6S9VmVkacAfQhkh/uA+HHEkkoVpnZfDod0bw+399yH2vfMTbqzbTPCOdkjVbmH7d1+jXuU3YEaWJiXcPgJPNbIOZlVSaPtrMlprZcjObWMtqxgBdgL1E+isXaXIy0tOYeOphTLlsGBu27eazrbsw4GldvpIQxPtS1RRgdPQEM0sHJgGnAv2BcWbW38wON7PplYaOQF/gDXf/PnBNnPOKJLVRfTvy2k0n8MqPCzm2Vwf+uXAd7uqKVhIrroXD3WcDlVtuGw4sd/cV7r4HKALGuPsidz+j0rCByFnG5mBZNR8qTV6LzHQy0tM444jOlG7eyXulW8KOJE2MxfuvFTPLB6a7+8Bg/BxgtLtfGYxfBIxw9wnVLN8SuBvYASxx90nVzDceGA+Qm5tbUFRUVK+8ZWVlZGdn12vZeErWXJC82VI91/a9zg0v7WBQx3QuHdCc1pkH1n95su4vSN5sqZarsLBwgbsPrXVGd4/rAOQDJVHj5wIPRI1fBNzdkNssKCjw+iouLq73svGUrLnckzdbU8j1qxmLvdtN073/T5/3Zeu3HdC6knV/uSdvtlTLReQBpFqPsWE8jlsK5EWNdwXWhpBDpNGbeOphvHDjcezd5zz61qqw40gTEUbhmAf0NrPuZpYJjAWeDSGHSEo4rFMbTuqfy9PvrFGz7JIQ8X4cdyrwBtDXzErN7Ap3LwcmADOBxcA0d38/njlEUt05Q7uyecdeZi1eH3YUaQLi+gVAdx9XzfQZwIx4blukKRnZ+2A6tcni7peWM7x7ew7Kbh52JElhanJEJAWkpxk/HzOAjz4r48x7XmPdlp1hR5IUpsIhkiJOGdCJaVcdzYZtu7jv5Y/CjiMpTIVDJIUMysthzOAuTJtfymY1gihxosIhkmLGj+zBzr37+NXzi5m1eD3l+/SklTQsFQ6RFNMntzUn989l2vxSrnh4Pr+buTTsSJJiVDhEUtDdFxzJrB8cz9lHduHBOR/z4fptYUeSFJL0/XGISOyaN0un58HZ3HpGf15auoGx979J+1aZNS6zY/sOWr79SoISxiZZsyVjrj+cNyju21DhEElh7Vtlcu8FQ3j0rU9qnXfDZzvpeHDrBKSKXbJmS8ZcLTLS474NFQ6RFHdMrw4c06tDrfO9/PLLjBo1JAGJYpes2ZI115rF8V2/7nGIiEhMVDhERCQmKhwiIhITFQ4REYmJCoeIiMREhUNERGKiwiEiIjFR4RARkZiYu4edocGZ2WfAqnou3gHY2IBxGkqy5oLkzaZcsUnWXJC82VItVzd3P7i2mVKycBwIM5vv7kPDzlFZsuaC5M2mXLFJ1lyQvNmaai5dqhIRkZiocIiISExUOL7q/rADVCNZc0HyZlOu2CRrLkjebE0yl+5xiIhITHTGISIiMVHhEBGRmKhwRDGz0Wa21MyWm9nEEHPkmVmxmS02s/fN7IZg+u1mtsbM3g2G00LIttLMFgXbnx9Ma29mL5rZsuDfdgnO1Ddqn7xrZlvN7Maw9peZTTazDWZWEjWtyn1kEXcFn7mFZha3XoGqyfU7M1sSbPspM8sJpueb2c6ofXdfgnNV+7szs5uD/bXUzE5JcK6/R2VaaWbvBtMTub+qOz4k7jPm7hoi93nSgY+AHkAm8B7QP6QsnYEhwevWwIdAf+B24Ich76eVQIdK034LTAxeTwR+E/Lv8VOgW1j7CxgJDAFKattHwGnA84ABRwFvJTjXyUCz4PVvonLlR88Xwv6q8ncX/D94D2gOdA/+z6YnKlel938P3BbC/qru+JCwz5jOOP5rOLDc3Ve4+x6gCBgTRhB3X+fubwevtwGLgS5hZKmjMcDDweuHgbNCzPJ14CN3r2/LAQfM3WcDmypNrm4fjQEe8Yg3gRwz65yoXO7+L3cvD0bfBLrGY9ux5qrBGKDI3Xe7+8fAciL/dxOay8wMOA+YGo9t16SG40PCPmMqHP/VBVgdNV5KEhyszSwfOBJ4K5g0ITjdnJzoS0IBB/5lZgvMbHwwLdfd10HkQw10DCFXhbF8+T9z2PurQnX7KJk+d5cT+cu0Qncze8fMXjGz40LIU9XvLln213HAendfFjUt4fur0vEhYZ8xFY7/siqmhfqsspllA/8AbnT3rcCfgJ7AYGAdkVPlRDvW3YcApwLfNbORIWSokpllAmcCjweTkmF/1SYpPndmdgtQDjwaTFoHHOruRwLfBx4zszYJjFTd7y4p9hcwji//gZLw/VXF8aHaWauYdkD7TIXjv0qBvKjxrsDakLJgZhlEPhSPuvuTAO6+3t33uft+4C/E6RS9Ju6+Nvh3A/BUkGF9xalv8O+GROcKnAq87e7rg4yh768o1e2j0D93ZnYJcAZwoQcXxYNLQZ8HrxcQuZfQJ1GZavjdJcP+agacDfy9Ylqi91dVxwcS+BlT4fiveUBvM+se/OU6Fng2jCDB9dMHgcXu/oeo6dHXJb8JlFReNs65WplZ64rXRG6slhDZT5cEs10CPJPIXFG+9Fdg2Purkur20bPAxcGTL0cBWyouNySCmY0GbgLOdPcdUdMPNrP04HUPoDewIoG5qvvdPQuMNbPmZtY9yDU3UbkCJwJL3L20YkIi91d1xwcS+RlLxFMAjWUg8vTBh0T+WrglxBxfI3IquRB4NxhOA/4KLAqmPwt0TnCuHkSeaHkPeL9iHwEHAbOAZcG/7UPYZy2Bz4G2UdNC2V9Eitc6YC+Rv/auqG4fEbmMMCn4zC0ChiY413Ii178rPmf3BfN+K/gdvwe8DXwjwbmq/d0BtwT7aylwaiJzBdOnAFdXmjeR+6u640PCPmNqckRERGKiS1UiIhITFQ4REYmJCoeIiMREhUNERGKiwiEiIjFR4ZBGwczczH4fNf5DM7u9gdY9xczOaYh11bKdc4MWTYsrTT/EzJ4IXg+2BmzF18xyzOzaqrYlUl8qHNJY7AbONrMOYQeJVvGlrzq6ArjW3QujJ7r7WnevKFyDiTyTH0uGZjW8nQP8p3BU2pZIvahwSGNRTqQf5e9VfqPyGYOZlQX/jgoanJtmZh+a2a/N7EIzm2uRPkV6Rq3mRDN7NZjvjGD5dIv0VzEvaGzvqqj1FpvZY0S+UFU5z7hg/SVm9ptg2m1Evrh1n5n9rtL8+cG8mcAvgPMt0qfD+cG39ScHGd4xszHBMpea2eNm9hyRRiezzWyWmb0dbLuiZedfAz2D9f2uYlvBOrLM7KFg/nfMrDBq3U+a2QsW6dvht1H7Y0qQdZGZfeV3IU1DTX+piCSbScDCigNZHQ0C+hFpHnsF8IC7D7dI5zfXATcG8+UDxxNpWK/YzHoBFxNpnmGYmTUHXjOzfwXzDwcGeqRp7/8ws0OI9GtRAGwmclA/y91/YWYnEOljYn5VQd19T1Bghrr7hGB9vwRecvfLLdLJ0lwz+3ewyNHAEe6+KTjr+Ka7bw3Oyt40s2eJ9Msw0N0HB+vLj9rkd4PtHm5mhwVZK9pXGkyk1dXdwFIzu5tIa6td3H1gsK6cmne9pCqdcUij4ZEWQB8Bro9hsXke6b9gN5EmFyoO/IuIFIsK09x9v0eayV4BHEakLa6LLdLL21tEmnToHcw/t3LRCAwDXnb3zzzSz8WjRDoEqq+TgYlBhpeBLODQ4L0X3b2ivwgDfmlmC4F/E2k2O7eWdX+NSNMeuPsSYBX/bZhvlrtvcfddwAdEOsZaAfQws7uDNq5qapFVUpjOOKSx+T8ibQE9FDWtnOCPoKABuMyo93ZHvd4fNb6fL3/+K7e940QOxte5+8zoN8xsFLC9mnxVNWF9IAz4lrsvrZRhRKUMFwIHAwXuvtfMVhIpMrWtuzrR+20fkV4CN5vZIOAUImcr5xHpw0OaGJ1xSKMS/IU9jciN5goriVwagkhvZxn1WPW5ZpYW3PfoQaQBvZnANRZpwhoz62ORVoFr8hZwvJl1CG6cjwNeiSHHNiLdgVaYCVwXFETM7MhqlmsLbAiKRiGRM4Sq1hdtNpGCQ3CJ6lAiP3eVgktgae7+D+CnRLpVlSZIhUMao98D0U9X/YXIwXouUPkv8bpaSuQA/zyRlk93AQ8QuUzzdnBD+c/UcpbukeaqbwaKCVpKdfdYmpkvBvpX3BwH7iBSCBcGGe6oZrlHgaFmNp9IMVgS5PmcyL2Zkso35YF7gXQzW0Skb4lLg0t61ekCvBxcNpsS/JzSBKl1XBERiYnOOEREJCYqHCIiEhMVDhERiYkKh4iIxESFQ0REYqLCISIiMVHhEBGRmPx/LwlufkF9AW4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52f79abf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of solution: 0.11%\n"
     ]
    }
   ],
   "source": [
    "'''from Fabian Pedregosa'''\n",
    "def FW(alpha, max_iter=200, tol=1e-8):\n",
    "    # .. initial estimate, could be any feasible point ..\n",
    "    x_t = sparse.dok_matrix((n_features, 1))\n",
    "    trace = []  # to keep track of the gap\n",
    "\n",
    "    # .. some quantities can be precomputed ..\n",
    "    Atb = A.T.dot(b)\n",
    "    for it in range(max_iter):\n",
    "        # .. compute gradient. Slightly more involved than usual because ..\n",
    "        # .. of the use of sparse matrices ..\n",
    "        Ax = x_t.T.dot(A.T).ravel()\n",
    "        grad = (A.T.dot(Ax) - Atb)\n",
    "        # .. the LMO results in a vector that is zero everywhere except for ..\n",
    "        # .. a single index. Of this vector we only store its index and magnitude ..\n",
    "        idx_oracle = np.argmax(np.abs(grad))\n",
    "        mag_oracle = alpha * np.sign(-grad[idx_oracle])\n",
    "        g_t = x_t.T.dot(grad).ravel() - grad[idx_oracle] * mag_oracle\n",
    "        trace.append(g_t)\n",
    "        if g_t <= tol:\n",
    "            break\n",
    "        q_t = A[:, idx_oracle] * mag_oracle - Ax\n",
    "        step_size = min(q_t.dot(b - Ax) / q_t.dot(q_t), 1.)\n",
    "        x_t = (1. - step_size) * x_t\n",
    "        x_t[idx_oracle] = x_t[idx_oracle] + step_size * mag_oracle\n",
    "    return x_t, np.array(trace)\n",
    "\n",
    "n_samples, n_features = 1000, 10000\n",
    "#n_samples, n_features = 100, 1000\n",
    "A, b = datasets.make_regression(n_samples, n_features)\n",
    "# .. plot evolution of FW gap ..\n",
    "sol, trace = FW(.5 * n_features)\n",
    "#print(trace)\n",
    "plt.plot(trace)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('FW gap')\n",
    "plt.title('FW on a Lasso problem')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "sparsity = np.mean(sol.toarray().ravel() != 0)\n",
    "print('Sparsity of solution: %s%%' % (sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([], dtype=float64)\n",
      "     fun: -11.428571428000076\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 4\n",
      "   slack: array([1.9874058e-10, 2.5518343e-10])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-1.14285714,  2.57142857])\n",
      "11.428571428000076\n"
     ]
    }
   ],
   "source": [
    "'''Linear Programming'''\n",
    "# f(x0,x1) = -x0 + 4x1\n",
    "f = lambda x: -x[0] + 4*x[1]\n",
    "c = np.array([-1, 4])\n",
    "A = [[-3, 1], [1, 2]]\n",
    "b = [6, 4]\n",
    "\n",
    "#A = np.array([[-3, 1]])\n",
    "#b = 6\n",
    "\n",
    "x0_bounds = (None, None)\n",
    "x1_bounds = (-3, None)\n",
    "res = linprog(-c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds]) #there is also A_eq and b_eq which is Ax = b constraints\n",
    "print(res)\n",
    "print(f(res[\"x\"]))\n",
    "#print(linear_program_oracle(c, A, b, [(None, None), (-3, None)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KWSA [-32.  -8.]\n",
      "RDSA [-86.06937832  29.24470557]\n",
      "IRDSA [-52.53287236  12.8793041 ]\n",
      "KWSA [0. 0.]\n",
      "RDSA [-0.  0.]\n",
      "IRDSA [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "'''Helper functions, for gradients, oracles, etc'''\n",
    "def grad_KWSA(F, x, c, d, *f_args):\n",
    "    '''\n",
    "    f_args is the arguments of F(.), used if there are complicated functions which reqiures more parameter than just x \n",
    "    '''\n",
    "    grad = 0\n",
    "    e = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        e[i] = 1 #cb-vector\n",
    "        grad += ((F(x + c*e, *f_args) - F(x, *f_args))/c)*e #calculate the gradient\n",
    "        e[i] = 0 #reset cb-vector\n",
    "    return grad\n",
    "\n",
    "def grad_RDSA(F, x, c, d, *f_args):\n",
    "    z = np.random.normal(0, 1, size=(d)) #mean = 0, std = 1\n",
    "    grad = ((F(x + c*z, *f_args) - F(x, *f_args))/c)*z\n",
    "    return grad\n",
    "\n",
    "def grad_IRDSA(F, x, c, d, m=5, *f_args):\n",
    "    grad = 0\n",
    "    for i in range(m):\n",
    "        z = np.random.normal(0, 1, size=(d)) #mean = 0, std = 1\n",
    "        grad += ((F(x + c*z, *f_args) - F(x, *f_args))/c)*z\n",
    "    return grad/m\n",
    "\n",
    "def linear_program_oracle(coef, A_ub, b_ub, bounds):\n",
    "    '''\n",
    "    f(x) = c^Tx\n",
    "    Ax <= b\n",
    "    x within bounds \n",
    "    coef = [c_x0, c_x1, ....] coefficient of the equation\n",
    "    A = [[a0_x0, a0_x1, ...],[a1_x0, a1_x1, ...]] ineq constraint matrix\n",
    "    b = [b0, b1, ....] ineq constraint vector\n",
    "    bounds = [(lb_x0, ub_x0), (lb_x1, ub_x1),....]\n",
    "    '''\n",
    "    res = linprog(coef, A_ub=A_ub, b_ub=b_ub, bounds=bounds)\n",
    "    return res[\"x\"] \n",
    "\n",
    "# gradient test:\n",
    "'''\n",
    "f = lambda x: x[0]**2 + x[1]**2\n",
    "c = 1e-10\n",
    "x = np.array([1,2])\n",
    "'''\n",
    "f = lambda x: x[0]**4 - 32*x[0] + x[1]**2 - 8*x[1]\n",
    "c = 1e-10\n",
    "x = np.array([0,0])\n",
    "print(\"KWSA\", grad_KWSA(f, x, c, len(x)))\n",
    "print(\"RDSA\", grad_RDSA(f, x, c, len(x)))\n",
    "print(\"IRDSA\", grad_IRDSA(f, x, c, len(x)))\n",
    "x = np.array([2,4]) #x = (2,4) is the global minimum, hence gradient = 0\n",
    "print(\"KWSA\", grad_KWSA(f, x, c, len(x)))\n",
    "print(\"RDSA\", grad_RDSA(f, x, c, len(x)))\n",
    "print(\"IRDSA\", grad_IRDSA(f, x, c, len(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Franke Wolfe Algorithm and its variants from [Sahu et al] paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Standard frank wolfe algorfithm'''\n",
    "def frank_wolfe(F, x, LPO_var, alpha=0.1, max_iter=200, *f_args):\n",
    "    '''\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    f_args is the arguments of F(.), used if there are complicated functions which reqiures more parameter than just x \n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    x_out = np.copy(x) #so that the global input x wont be changed\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_KWSA(F, x_out, 1e-10, dim, *f_args)\n",
    "        v = linear_program_oracle(grad, A, b, bounds)\n",
    "        x_out += alpha*(v - x_out)\n",
    "    return x_out\n",
    "\n",
    "def frank_wolfe_away(F, x, LPO_var, alpha=0.1, max_iter=200, *f_args):\n",
    "    '''\n",
    "    away version of Frank Wolfe Algorithm\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    alpha_bar = 0 #placeholder\n",
    "    x_out = np.copy(x)\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_KWSA(F, x_out, 1e-10, dim, *f_args)\n",
    "        v_FW = linear_program_oracle(grad, A, b, bounds)\n",
    "        v_AS = linear_program_oracle(-grad, A, b, bounds) #away step\n",
    "        d_FW = v_FW - x_out; d_AS = x_out - v_AS\n",
    "        grad_FW = np.dot(grad, d_FW); grad_AS = np.dot(grad, d_AS)\n",
    "        if grad_FW <= grad_AS:\n",
    "            d = d_FW\n",
    "            alpha_bar = 1\n",
    "        else:\n",
    "            d = d_AS\n",
    "            alpha_bar = 1 #temporary\n",
    "        x_out += alpha*d\n",
    "    return x_out\n",
    "    \n",
    "    \n",
    "'''Zeroth Order Frank Wolfe Algorithms [Sahu et al]'''\n",
    "'''Stochastic Gradient Free Frank Wolfe - Convex'''\n",
    "def SGF_frank_wolfe_cvx(F, x, LPO_var, max_iter=200, m=10, mode=\"KWSA\", *f_args):\n",
    "    '''\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    x_out = np.copy(x)\n",
    "    t = np.arange(max_iter) # sequence of t = 0,1,....max_iter\n",
    "    gamma = 2/(t + 8) # initialize gamma sequences\n",
    "    # rho and c sequences:\n",
    "    rho = c = None\n",
    "    if mode == \"KWSA\":\n",
    "        rho = 4/((t+8)**(2/3))\n",
    "        c = 2/(dim**(1/2) * ((t + 8)**(1/3)))\n",
    "    elif mode == \"IRDSA\":\n",
    "        rho = 4/(((1 + (dim/m))**(1/3)) * ((t+8)**(2/3)))\n",
    "        c = 2*np.sqrt(m)/(dim**(3/2) * (t+8)**(1/3))\n",
    "    elif mode == \"RDSA\":\n",
    "        rho = 4/(dim**(1/3) * (t+8)**(2/3))\n",
    "        c = 2/(dim**(3/2) * (t+8)**(1/3))\n",
    "    d = np.zeros(dim) #initial value d_0 = 0\n",
    "    x_mean = 0 #for xT\n",
    "    for i in t: #loop\n",
    "        grad = 0 #placeholder\n",
    "        if mode == \"KWSA\":\n",
    "            grad = grad_KWSA(F, x_out, c[i], dim, *f_args)\n",
    "        elif mode == \"IRDSA\":\n",
    "            grad = grad_IRDSA(F, x_out, c[i], dim, m, *f_args)\n",
    "        elif mode == \"RDSA\":\n",
    "            grad = grad_RDSA(F, x_out, c[i], dim, *f_args)\n",
    "        d = (1-rho[i])*d + rho[i]*grad #used for linear programming oracle (LPO)\n",
    "        v = linear_program_oracle(d, A, b, bounds) ### LPO\n",
    "        x_out = (1-gamma[i])*x_out + gamma[i]*v\n",
    "        x_mean += x_out\n",
    "    x_mean /= max_iter #(sum of x)/T\n",
    "    return x_mean\n",
    "\n",
    "'''Stochastic Gradient Free Frank Wolfe - NonConvex'''\n",
    "def SGF_frank_wolfe_noncvx(F, x, LPO_var, max_iter=200, m=10, *f_args):\n",
    "    '''\n",
    "    F = f(x)\n",
    "    x = input vector or init starting point\n",
    "    LPO_var = {A, b, bounds}, the constraints and bounds\n",
    "    '''\n",
    "    A = LPO_var[\"A\"]; b = LPO_var[\"b\"]; bounds = LPO_var[\"bounds\"]\n",
    "    dim = len(x)\n",
    "    x_out = np.copy(x)\n",
    "    t = np.arange(max_iter) # sequence of t = 0,1,....max_iter\n",
    "    gamma = 1/(max_iter**(3/4)) # initialize gamma sequences\n",
    "    # rho and c sequences:\n",
    "    rho = 4/((1+dim/m)**(1/3) * (t+8)**(2/3))\n",
    "    c = 2*np.sqrt(m)/(dim**(3/2) * (t+8)**(1/3))\n",
    "    d = np.zeros(dim) #initial value d_0 = 0\n",
    "    for i in t: #loop\n",
    "        grad = grad_IRDSA(F, x_out, c[i], dim, m, *f_args)\n",
    "        d = (1-rho[i])*d + rho[i]*grad #used for linear programming oracle (LPO)\n",
    "        v = linear_program_oracle(d, A, b, bounds) ### LPO\n",
    "        x_out = (1-gamma)*x_out + gamma*v\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Simple function for testing <br>\n",
    "$x^2 + y^2 + z^2$ <br>\n",
    "$-1 \\leq (x,y,z) \\leq 1$ <br>\n",
    "minimizer value:  $F(0,0,0) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{min x, F(x)} of Standard Frank Wolfe = [ 0.05263158 -0.05263158  0.05263158] 0.008310249311776426\n",
      "{min x, F(x)} of Frank Wolfe Away = [ 0.05263158 -0.05263158  0.05263158] 0.008310249311776426\n",
      "{min x, F(x)} of SGF Cvx Frank Wolfe = [ 0.01864734 -0.05487923 -0.00125604] 0.0033610305958132088\n",
      "{min x, F(x)} of SGF Non-Cvx Frank Wolfe = [-0.13396247  0.00948164 -0.0383157 ] 0.019503938360854525\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.sum(x**2)\n",
    "x = np.array([1., -1., 1.]) #random x0 within feasible reigon\n",
    "A = None\n",
    "b = None\n",
    "bounds = [(-1, 1)]*len(x)\n",
    "LPO_var = {\"A\":A, \"b\":b, \"bounds\":bounds}\n",
    "m=3 #for IRDSA gradient\n",
    "\n",
    "x_out = frank_wolfe(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Standard Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = frank_wolfe_away(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Frank Wolfe Away =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_cvx(f, x, LPO_var, max_iter=200, m=m, mode=\"IRDSA\") #looks like m corresponds to the dimension size of the problem to be more accurate\n",
    "print(\"{min x, F(x)} of SGF Cvx Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, max_iter=200, m=m)\n",
    "print(\"{min x, F(x)} of SGF Non-Cvx Frank Wolfe =\", x_out, f(x_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximization test, $f = -F$ <br>\n",
    "$F(x,y,z) = x^2 + y^2 + z^2$ <br>\n",
    "$-1 \\leq (x,y,z) \\leq 1$ <br>\n",
    "the combinations of $\\{-1,1\\} \\in \\mathbb{Z}^3$ are the max values, e.g: $F(-1,1,1) = -3,\\quad f = -F = 3$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{min x, F(x)} of Standard Frank Wolfe = [1. 1. 1.] 2.9999999957669528\n",
      "{min x, F(x)} of Frank Wolfe Away = [842333.73448923 842333.73448923 842335.23367636] 2128580886409.752\n",
      "{min x, F(x)} of SGF Cvx Frank Wolfe = [ 0.97101449  0.82173913 -0.7857971 ] 2.2356014282713703\n",
      "{min x, F(x)} of SGF Non-Cvx Frank Wolfe = [0.97650838 0.97563416 0.97183358] 2.8498911184981144\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "f = lambda x: -np.sum(x**2)\n",
    "x = np.array([0., 0., 0.]) #random x0 within feasible reigon\n",
    "A = None\n",
    "b = None\n",
    "bounds = [(-1, 1)]*len(x)\n",
    "LPO_var = {\"A\":A, \"b\":b, \"bounds\":bounds}\n",
    "m=3 #for IRDSA gradient\n",
    "\n",
    "x_out = frank_wolfe(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Standard Frank Wolfe =\", x_out, -f(x_out))\n",
    "x_out = frank_wolfe_away(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Frank Wolfe Away =\", x_out, -f(x_out))\n",
    "x_out = SGF_frank_wolfe_cvx(f, x, LPO_var, max_iter=200, m=m, mode=\"IRDSA\") #looks like m corresponds to the dimension size of the problem to be more accurate\n",
    "print(\"{min x, F(x)} of SGF Cvx Frank Wolfe =\", x_out, -f(x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, max_iter=200, m=m)\n",
    "print(\"{min x, F(x)} of SGF Non-Cvx Frank Wolfe =\", x_out, -f(x_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function from: http://www.math.udel.edu/~angell/Opt/FW.pdf <br>\n",
    "minimzer value: $ F(1,695, 1,914) = -57.63$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{min x, F(x)} of Standard Frank Wolfe = [1.582 2.253] -57.31468003018529\n",
      "{min x, F(x)} of Frank Wolfe Away = [2.178 3.739] -63.12670634345123\n",
      "{min x, F(x)} of SGF Cvx Frank Wolfe = [1.449 2.352] -55.23591041598563\n",
      "{min x, F(x)} of SGF Non-Cvx Frank Wolfe = [1.549 2.195] -56.55772389787758\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x[0]**4 - 32*x[0] + x[1]**2 - 8*x[1]\n",
    "x = np.array([0., 0.])\n",
    "A = [[1, -1], [3, 1]]\n",
    "b = [1, 7]\n",
    "bounds = [(0, None), (0, None)]\n",
    "LPO_var = {\"A\":A, \"b\":b, \"bounds\":bounds}\n",
    "m=5 #for IRDSA gradient\n",
    "x_out = frank_wolfe(f, x, LPO_var)\n",
    "print(\"{min x, F(x)} of Standard Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = frank_wolfe_away(f, x, LPO_var, max_iter=200)\n",
    "print(\"{min x, F(x)} of Frank Wolfe Away =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_cvx(f, x, LPO_var, max_iter=200, m=m, mode=\"IRDSA\") #looks like m corresponds to the dimension size of the problem to be more accurate\n",
    "print(\"{min x, F(x)} of SGF Cvx Frank Wolfe =\", x_out, f(x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, max_iter=200, m=m)\n",
    "print(\"{min x, F(x)} of SGF Non-Cvx Frank Wolfe =\", x_out, f(x_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression test, treated as blackbox function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal function value (residual) =  1.464914700995419e-26\n",
      "f(x) of Standard Frank Wolfe = 605637.5516793226\n",
      "||coef - x_out|| = 126.35263873557528\n",
      "f(x) of SGF Non-Cvx FW = 613824.4439723756\n",
      "||coef - x_out|| = 127.1079372798198\n",
      "f(lasso_coef) = 612.3887673018115\n",
      "||coef-lasso_coef|| = 4.10269643375423\n"
     ]
    }
   ],
   "source": [
    "#Regression test, treated as black box\n",
    "float_formatter = \"{:.3f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "n_samples = 100; n_features = 100 #A is n_samples x n_features matrix, x is the variable to be optimized, b is the target\n",
    "A, b, coef = datasets.make_regression(n_samples, n_features, coef=True, random_state=0) #coef is the optimizer value\n",
    "f = lambda x,A,b: 0.5*np.square(np.linalg.norm(np.dot(A,x) - b))\n",
    "#f = lambda x,A,b: (0.5/b.shape[0])*np.square(np.linalg.norm(np.dot(A,x) - b)) + np.linalg.norm(x,1)\n",
    "#print(\"optimal coefficients = \",coef)\n",
    "print(\"optimal function value (residual) = \", f(coef, A, b)) #the optimal value should be near 0\n",
    "A_ub = np.ones((1,n_features)) #L1 norm of x is <= 0.5*n_features\n",
    "b_ub = 0.5*n_features\n",
    "#A_ub = None\n",
    "#b_ub = None\n",
    "\n",
    "'''\n",
    "#gradient test:\n",
    "#print(\"KWSA\",grad_KWSA(f, np.zeros(n_features), 1e-10, n_features, A,b))\n",
    "#print(\"RDSA\",grad_KWSA(f, np.zeros(n_features), 1e-10, n_features, A,b))\n",
    "#print(\"IRDSA\",grad_IRDSA(f, np.zeros(n_features), 1e-10, n_features, 5, A,b))\n",
    "'''\n",
    "\n",
    "\n",
    "LPO_var = {\"A\":A_ub, \"b\":b_ub, \"bounds\":np.array([[0, None]]*n_features)}\n",
    "m=6\n",
    "x = np.random.normal(0, 1, size=(n_features))\n",
    "x_out = frank_wolfe(f, x, LPO_var, 0.1, 200, A, b)\n",
    "#print(\"x of Standard Frank Wolfe =\", x_out)\n",
    "print(\"f(x) of Standard Frank Wolfe =\", f(x_out, A, b))\n",
    "print(\"||coef - x_out|| =\", np.linalg.norm(coef-x_out))\n",
    "x_out = SGF_frank_wolfe_noncvx(f, x, LPO_var, 200, m, A, b)\n",
    "#print(\"x of SGF Non-Cvx FW =\", x_out)\n",
    "print(\"f(x) of SGF Non-Cvx FW =\", f(x_out, A, b))\n",
    "print(\"||coef - x_out|| =\", np.linalg.norm(coef-x_out))\n",
    "\n",
    "# actual lasso fit:\n",
    "from sklearn.linear_model import Lasso\n",
    "alpha=1\n",
    "dense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\n",
    "dense_lasso.fit(A, b)\n",
    "print(\"f(lasso_coef) =\", f(dense_lasso.coef_,A,b))\n",
    "print(\"||coef-lasso_coef|| =\", np.linalg.norm(coef-dense_lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal value is -15.220912605552863\n",
      "A solution x is\n",
      "[-1.10133381 -0.16360111 -0.89734939  0.03216603  0.6069123  -1.12687348\n",
      "  1.12967856  0.88176638  0.49075229  0.8984822 ]\n",
      "A dual solution is\n",
      "[6.98805172e-10 6.11756416e-01 5.28171747e-01 1.07296862e+00\n",
      " 3.93759300e-09 2.30153870e+00 4.25704434e-10 7.61206896e-01\n",
      " 8.36906030e-09 2.49370377e-01 1.30187120e-09 2.06014070e+00\n",
      " 3.22417207e-01 3.84054343e-01 1.59493839e-09]\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "# Generate a random non-trivial linear program.\n",
    "m = 15\n",
    "n = 10\n",
    "np.random.seed(1)\n",
    "s0 = np.random.randn(m)\n",
    "lamb0 = np.maximum(-s0, 0)\n",
    "s0 = np.maximum(s0, 0)\n",
    "x0 = np.random.randn(n)\n",
    "A = np.random.randn(m, n)\n",
    "b = A @ x0 + s0\n",
    "c = -A.T @ lamb0\n",
    "\n",
    "# Define and solve the CVXPY problem.\n",
    "x = cp.Variable(n)\n",
    "prob = cp.Problem(cp.Minimize(c.T@x),\n",
    "                 [A @ x <= b])\n",
    "prob.solve()\n",
    "\n",
    "# Print result.\n",
    "print(\"\\nThe optimal value is\", prob.value)\n",
    "print(\"A solution x is\")\n",
    "print(x.value)\n",
    "print(\"A dual solution is\")\n",
    "print(prob.constraints[0].dual_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
